{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igl4v6gmfhQz"
      },
      "source": [
        "Environment Setup:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cell 1:"
      ],
      "metadata": {
        "id": "3KHFhM6_5ObZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU9q6-UufkWi",
        "outputId": "62eed635-e303-4ba8-b8d0-7214812dd967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/49.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-text-splitters) (1.2.6)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.6.1)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.13.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.5.0)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.2/57.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m935.6/935.6 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m134.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m134.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# %%capture\n",
        "\n",
        "%pip install -q langchain langchain-core langchain-nvidia-ai-endpoints gradio rich\n",
        "%pip install -q arxiv pymupdf faiss-cpu langchain-community\n",
        "%pip install -U langchain-text-splitters\n",
        "\n",
        "!pip install -qU pennylane pennylane-lightning scikit-learn\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "ChatNVIDIA.get_available_models(api_key=userdata.get(\"NVIDIA_API_KEY\"))\n",
        "\n",
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=base_style)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import pennylane as qml\n",
        "import sklearn\n",
        "import torch\n",
        "\n",
        "def verify_quantum_env():\n",
        "    print(\"---Quantum Foundation Check ---\")\n",
        "\n",
        "    # Check 1: Libraries\n",
        "    print(f\"Python version: {sys.version.split()[0]}\")\n",
        "    print(f\"PennyLane (Quantum Lib): {qml.__version__}\")\n",
        "    print(f\"Scikit-learn (for PCA): {sklearn.__version__}\")\n",
        "\n",
        "    # Check 2: Quantum Device Initialization\n",
        "    # We'll try to create a simple 4-qubit\n",
        "    try:\n",
        "        dev = qml.device(\"default.qubit\", wires=4)\n",
        "        print(\"✅ Quantum Simulator: Operational\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Quantum Simulator Error: {e}\")\n",
        "\n",
        "    # Check 3: PyTorch Integration\n",
        "    print(f\"✅ PyTorch (Classical backend): {torch.__version__}\")\n",
        "    print(\"----------------------------------\")\n",
        "\n",
        "verify_quantum_env()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oujSBW2Bl7g",
        "outputId": "197306b7-cf39-484a-82a3-16ab96633c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Quantum Foundation Check ---\n",
            "Python version: 3.12.12\n",
            "PennyLane (Quantum Lib): 0.44.0\n",
            "Scikit-learn (for PCA): 1.8.0\n",
            "✅ Quantum Simulator: Operational\n",
            "✅ PyTorch (Classical backend): 2.9.0+cpu\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cell 2:"
      ],
      "metadata": {
        "id": "0tzV52M85TNA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJKNgj2DfoAA"
      },
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "from google.colab import userdata\n",
        "\n",
        "#NVIDIAEmbeddings.get_available_models()\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\",api_key=userdata.get(\"NVIDIA_API_KEY\"))\n",
        "\n",
        "#ChatNVIDIA.get_available_models()\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\",api_key=userdata.get(\"NVIDIA_API_KEY\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "def verify_classical_rag():\n",
        "    print(\"---NVIDIA AI Stack Check ---\")\n",
        "\n",
        "    try:\n",
        "        # 1. Initialize Embedder\n",
        "\n",
        "        embedder = NVIDIAEmbeddings(\n",
        "            model=\"nvidia/nv-embed-v1\",\n",
        "            truncate=\"END\",\n",
        "            api_key=userdata.get(\"NVIDIA_API_KEY\")\n",
        "        )\n",
        "\n",
        "        # Test Embedding\n",
        "        test_text = \"What is a hybrid quantum neural network?\"\n",
        "        query_result = embedder.embed_query(test_text)\n",
        "        print(f\"✅ Embedder: Ready (Vector dimension: {len(query_result)})\")\n",
        "\n",
        "        # 2. Initialize LLM\n",
        "\n",
        "        instruct_llm = ChatNVIDIA(\n",
        "            model=\"mistralai/mixtral-8x22b-instruct-v0.1\",\n",
        "            api_key=userdata.get(\"NVIDIA_API_KEY\")\n",
        "        )\n",
        "\n",
        "        # Test LLM\n",
        "        res = instruct_llm.invoke(\"Say 'Classical RAG is active'\")\n",
        "        print(f\"✅ LLM: Ready (Response: '{res.content.strip()}')\")\n",
        "\n",
        "        return embedder, instruct_llm\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌Stack Error: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Run verification and keep models for next steps\n",
        "embedder, instruct_llm = verify_classical_rag()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnH7NCv8DCJJ",
        "outputId": "9f2ccd02-422c-44f2-8554-6031e0721968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---NVIDIA AI Stack Check ---\n",
            "✅ Embedder: Ready (Vector dimension: 4096)\n",
            "✅ LLM: Ready (Response: '\"Classical RAG is active\" - It seems like there's a specific context missing here, but I can assure you that I'm actively ready to assist you with any questions or information you need regarding classical music or any other topic.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TUmzPiRT5SVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **We** will compress your 4096-dimensional NVIDIA embeddings into 16 dimensions. This allows us to use 16 qubits on your IBM hardware later."
      ],
      "metadata": {
        "id": "DKFEDzf2Uhin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "class QuantumDataBridge:\n",
        "    def __init__(self, n_qubits=16):\n",
        "        self.n_qubits = n_qubits\n",
        "        self.pca = PCA(n_components=n_qubits)\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def fit(self, embeddings):\n",
        "        \"\"\"Train the PCA on your document corpus embeddings\"\"\"\n",
        "        print(f\"Fitting Quantum Bridge: 4096 dims -> {self.n_qubits} qubits\")\n",
        "        # Ensure input is a numpy array\n",
        "        embeddings_array = np.array(embeddings)\n",
        "        self.pca.fit(embeddings_array)\n",
        "        self.is_fitted = True\n",
        "        print(\"✅ Quantum Bridge: Successfully fitted to data.\")\n",
        "\n",
        "    def transform(self, embeddings):\n",
        "        \"\"\"Compress embeddings and map to quantum rotation angles [0, pi]\"\"\"\n",
        "        if not self.is_fitted:\n",
        "            raise ValueError(\"Bridge must be fitted before transform! Run .fit() first.\")\n",
        "\n",
        "        reduced_data = self.pca.transform(embeddings)\n",
        "\n",
        "        # Min-Max Scaling to map data to [0, pi] for Angle Embedding\n",
        "        #gate rotation\n",
        "        min_vals = reduced_data.min(axis=0)\n",
        "        max_vals = reduced_data.max(axis=0)\n",
        "\n",
        "        denom = (max_vals - min_vals) + 1e-9\n",
        "        norm_data = np.pi * (reduced_data - min_vals) / denom\n",
        "\n",
        "        return norm_data\n",
        "\n",
        "# --- Verification of Step 3 ---\n",
        "def verify_step_3():\n",
        "    # Simulate 50 document embeddings from your NVIDIA model (4096 dims)\n",
        "    mock_data = np.random.rand(50, 4096)\n",
        "\n",
        "    bridge = QuantumDataBridge(n_qubits=16)\n",
        "    bridge.fit(mock_data)\n",
        "\n",
        "    # Transform a single 'query' vector\n",
        "    quantum_query = bridge.transform(mock_data[0:1])\n",
        "\n",
        "    print(f\"Input Shape: {mock_data[0:1].shape}\")\n",
        "    print(f\"Quantum-Ready Shape: {quantum_query.shape}\")\n",
        "    print(f\"First 5 Qubit Angles: {quantum_query[0][:5]}\")\n",
        "\n",
        "    if quantum_query.shape == (1, 16) and np.max(quantum_query) <= np.pi:\n",
        "        print(\"✅ Step 3 Clear: Data is ready for the 16-qubit circuit.\")\n",
        "        return bridge\n",
        "    else:\n",
        "        print(\"❌ Step 3 Error: Dimensions or scaling incorrect.\")\n",
        "        return None\n",
        "\n",
        "bridge = verify_step_3()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txP_qpW9V1Rq",
        "outputId": "bb17195b-54db-41a8-deee-58b9b60555ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting Quantum Bridge: 4096 dims -> 16 qubits\n",
            "✅ Quantum Bridge: Successfully fitted to data.\n",
            "Input Shape: (1, 4096)\n",
            "Quantum-Ready Shape: (1, 16)\n",
            "First 5 Qubit Angles: [0. 0. 0. 0. 0.]\n",
            "✅ Step 3 Clear: Data is ready for the 16-qubit circuit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: The Quantum Kernel Implementation\n",
        "This is the \"heart\" of your project. We are going to build a Quantum Kernel Estimator. Instead of just measuring the angle between two vectors (classical cosine similarity), this circuit embeds your query and a document into a complex quantum state and measures their Overlap (Fidelity).\n",
        "\n",
        "Because we are using Angle Embedding, the features are mapped to rotations on the Bloch sphere. The kernel measures how closely these rotations align in a high-dimensional Hilbert space."
      ],
      "metadata": {
        "id": "QKX_dmRRWUqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from pennylane import numpy as pnp\n",
        "\n",
        "# We're setting up for our 16-qubit IBM hardware, but let's stick\n",
        "# with the simulator for now while we're testing the logic.\n",
        "n_qubits = 16\n",
        "dev = qml.device(\"default.qubit\", wires=n_qubits)\n",
        "\n",
        "@qml.qnode(dev)\n",
        "def quantum_kernel_circuit(x1, x2):\n",
        "    \"\"\"\n",
        "    This is our overlap circuit. We're encoding the query, then\n",
        "    running the inverse of the document chunk to see if they cancel out.\n",
        "    \"\"\"\n",
        "    # First, we encode our query vector\n",
        "    qml.AngleEmbedding(x1, wires=range(n_qubits), rotation='X')\n",
        "\n",
        "    # Now we apply the adjoint of our document encoding.\n",
        "    # If x1 and x2 are the same, this brings us back to the |0...0> state.\n",
        "\n",
        "    qml.adjoint(qml.AngleEmbedding)(x2, wires=range(n_qubits), rotation='X')\n",
        "\n",
        "    # We just need the probability of the zero state to get our similarity score.\n",
        "\n",
        "    return qml.probs(wires=range(n_qubits))\n",
        "\n",
        "def get_quantum_similarity(query_angles, doc_angles):\n",
        "    \"\"\"\n",
        "    Grabbing the first element of the probability vector, which\n",
        "    represents our |00...0> state (perfect overlap).\n",
        "    \"\"\"\n",
        "    probs = quantum_kernel_circuit(query_angles, doc_angles)\n",
        "    return float(probs[0])\n",
        "\n",
        "# --- Quick check to make sure our kernel logic holds up ---\n",
        "def verify_step_4():\n",
        "    print(\"---Checking Our Quantum Kernel ---\")\n",
        "\n",
        "    # If we pass in the same vector twice, our score should be 1.0\n",
        "    vec_a = pnp.array([np.pi/2] * 16)\n",
        "    sim_identical = get_quantum_similarity(vec_a, vec_a)\n",
        "    print(f\"Overlap (Same): {sim_identical:.4f}\")\n",
        "\n",
        "    # Different vectors should give us a much lower score\n",
        "    vec_b = pnp.array([0.0] * 16)\n",
        "    sim_different = get_quantum_similarity(vec_a, vec_b)\n",
        "    print(f\"Overlap (Different): {sim_different:.4f}\")\n",
        "\n",
        "    if sim_identical > 0.99:\n",
        "        print(\"✅ Step 4 Clear: Our kernel is calculating state overlap correctly.\")\n",
        "    else:\n",
        "        print(\"❌ Step 4 Error: Something is wrong with our similarity calculation.\")\n",
        "\n",
        "verify_step_4()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40qW0_czXDkT",
        "outputId": "0daa48be-742f-4516-92ec-1f775761ff7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Checking Our Quantum Kernel ---\n",
            "Overlap (Same): 1.0000\n",
            "Overlap (Different): 0.0000\n",
            "✅ Step 4 Clear: Our kernel is calculating state overlap correctly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6a9fOuZgUJ3"
      },
      "source": [
        "RAG For Document Chunk Retrieval:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MMVqy84g1eO"
      },
      "source": [
        "Task 1: Loading And Chunking Your Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vkTlSauzg8cf",
        "outputId": "3cbf044b-394b-463d-b92c-739a8b45f886"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Documents\n",
            "Chunking Documents\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Attention Is All You Need\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \u001b[0m\n",
              "\u001b[1;38;2;118;185;0msources and discrete reasoning\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Mistral 7B\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Large Language Models: A Survey \u001b[0m\n",
              "\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sources and discrete reasoning</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Mistral 7B</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Large Language Models: A Survey </span>\n",
              "\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 0\n",
            " - # Chunks: 35\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-08-02'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Attention Is All You Need'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz \u001b[0m\n",
              "\u001b[32mKaiser, Illia Polosukhin'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The dominant sequence transduction models are based on complex recurrent or convolutional neural \u001b[0m\n",
              "\u001b[32mnetworks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder \u001b[0m\n",
              "\u001b[32mthrough an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on \u001b[0m\n",
              "\u001b[32mattention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation \u001b[0m\n",
              "\u001b[32mtasks show these models to be superior in quality while being more parallelizable and requiring significantly less \u001b[0m\n",
              "\u001b[32mtime to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the \u001b[0m\n",
              "\u001b[32mexisting best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our \u001b[0m\n",
              "\u001b[32mmodel establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs,\u001b[0m\n",
              "\u001b[32ma small fraction of the training costs of the best models from the literature. We show that the Transformer \u001b[0m\n",
              "\u001b[32mgeneralizes well to other tasks by applying it successfully to English constituency parsing both with large and \u001b[0m\n",
              "\u001b[32mlimited training data.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-02'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Attention Is All You Need'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Kaiser, Illia Polosukhin'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The dominant sequence transduction models are based on complex recurrent or convolutional neural </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">tasks show these models to be superior in quality while being more parallelizable and requiring significantly less </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs,</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a small fraction of the training costs of the best models from the literature. We show that the Transformer </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">generalizes well to other tasks by applying it successfully to English constituency parsing both with large and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">limited training data.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 1\n",
            " - # Chunks: 45\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2019-05-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce a new language representation model called BERT, which stands for Bidirectional \u001b[0m\n",
              "\u001b[32mEncoder Representations from Transformers. Unlike recent language representation models, BERT is designed to \u001b[0m\n",
              "\u001b[32mpre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right \u001b[0m\n",
              "\u001b[32mcontext in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output \u001b[0m\n",
              "\u001b[32mlayer to create state-of-the-art models for a wide range of tasks, such as question answering and language \u001b[0m\n",
              "\u001b[32minference, without substantial task-specific architecture modifications.\\n  BERT is conceptually simple and \u001b[0m\n",
              "\u001b[32mempirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, \u001b[0m\n",
              "\u001b[32mincluding pushing the GLUE score to 80.5% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7.7% point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, MultiNLI accuracy to 86.7% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m4.6% \u001b[0m\n",
              "\u001b[32mabsolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, SQuAD v1.1 question answering Test F1 to 93.2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1.5 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and SQuAD \u001b[0m\n",
              "\u001b[32mv2.0 Test F1 to 83.1 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.1 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2019-05-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce a new language representation model called BERT, which stands for Bidirectional </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">layer to create state-of-the-art models for a wide range of tasks, such as question answering and language </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">inference, without substantial task-specific architecture modifications.\\n  BERT is conceptually simple and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">v2.0 Test F1 to 83.1 (5.1 point absolute improvement).'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 2\n",
            " - # Chunks: 46\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2021-04-12'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, \u001b[0m\n",
              "\u001b[32mHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Large pre-trained language models have been shown to store factual knowledge in their parameters, \u001b[0m\n",
              "\u001b[32mand achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and \u001b[0m\n",
              "\u001b[32mprecisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags \u001b[0m\n",
              "\u001b[32mbehind task-specific architectures. Additionally, providing provenance for their decisions and updating their world\u001b[0m\n",
              "\u001b[32mknowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit \u001b[0m\n",
              "\u001b[32mnon-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream \u001b[0m\n",
              "\u001b[32mtasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m -- models which \u001b[0m\n",
              "\u001b[32mcombine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the\u001b[0m\n",
              "\u001b[32mparametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of \u001b[0m\n",
              "\u001b[32mWikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on \u001b[0m\n",
              "\u001b[32mthe same retrieved passages across the whole generated sequence, the other can use different passages per token. We\u001b[0m\n",
              "\u001b[32mfine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on \u001b[0m\n",
              "\u001b[32mthree open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract \u001b[0m\n",
              "\u001b[32marchitectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual \u001b[0m\n",
              "\u001b[32mlanguage than a state-of-the-art parametric-only seq2seq baseline.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-04-12'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large pre-trained language models have been shown to store factual knowledge in their parameters, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the same retrieved passages across the whole generated sequence, the other can use different passages per token. We</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">language than a state-of-the-art parametric-only seq2seq baseline.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 3\n",
            " - # Chunks: 40\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2022-05-01'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external \u001b[0m\n",
              "\u001b[32mknowledge sources and discrete reasoning'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit\u001b[0m\n",
              "\u001b[32mBata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, \u001b[0m\n",
              "\u001b[32mAmnon Shashua, Moshe Tenenholtz'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Huge language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have ushered in a new era for AI, serving as a gateway to \u001b[0m\n",
              "\u001b[32mnatural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited\u001b[0m\n",
              "\u001b[32min a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. \u001b[0m\n",
              "\u001b[32mConceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we\u001b[0m\n",
              "\u001b[32mdefine a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning \u001b[0m\n",
              "\u001b[32mmodules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMRKL, \u001b[0m\n",
              "\u001b[32mpronounced \"miracle\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs\\' MRKL\u001b[0m\n",
              "\u001b[32msystem implementation.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-05-01'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge sources and discrete reasoning'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Amnon Shashua, Moshe Tenenholtz'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Huge language models (LMs) have ushered in a new era for AI, serving as a gateway to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">natural-language-based knowledge tasks. Although an essential element of modern AI, LMs are also inherently limited</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">in a number of ways. We discuss these limitations and how they can be avoided by adopting a systems approach. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Conceptualizing the challenge as one that involves knowledge and reasoning in addition to linguistic processing, we</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">define a flexible architecture with multiple neural models, complemented by discrete knowledge and reasoning </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">modules. We describe this neuro-symbolic architecture, dubbed the Modular Reasoning, Knowledge and Language (MRKL, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pronounced \"miracle\") system, some of the technical challenges in implementing it, and Jurassic-X, AI21 Labs\\' MRKL</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">system implementation.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 4\n",
            " - # Chunks: 21\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-10-10'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Mistral 7B'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
              "\u001b[32mDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, \u001b[0m\n",
              "\u001b[32mMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior \u001b[0m\n",
              "\u001b[32mperformance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in \u001b[0m\n",
              "\u001b[32mreasoning, mathematics, and code generation. Our model leverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for faster \u001b[0m\n",
              "\u001b[32minference, coupled with sliding window attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to effectively handle sequences of arbitrary length with a \u001b[0m\n",
              "\u001b[32mreduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that \u001b[0m\n",
              "\u001b[32msurpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the \u001b[0m\n",
              "\u001b[32mApache 2.0 license.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-10-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Mistral 7B'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Apache 2.0 license.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 5\n",
            " - # Chunks: 44\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-12-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, \u001b[0m\n",
              "\u001b[32mZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Evaluating large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m based chat assistants is challenging due to their broad \u001b[0m\n",
              "\u001b[32mcapabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore \u001b[0m\n",
              "\u001b[32musing strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and \u001b[0m\n",
              "\u001b[32mlimitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited \u001b[0m\n",
              "\u001b[32mreasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges \u001b[0m\n",
              "\u001b[32mand human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a \u001b[0m\n",
              "\u001b[32mcrowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and \u001b[0m\n",
              "\u001b[32mcrowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. \u001b[0m\n",
              "\u001b[32mHence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very \u001b[0m\n",
              "\u001b[32mexpensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by \u001b[0m\n",
              "\u001b[32mevaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations \u001b[0m\n",
              "\u001b[32mwith human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Evaluating large language model (LLM) based chat assistants is challenging due to their broad </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Document 6\n",
            " - # Chunks: 122\n",
            " - Metadata: \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2024-03-27'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Retrieval-Augmented Generation for Large Language Models: A Survey'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng \u001b[0m\n",
              "\u001b[32mWang, Haofen Wang'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m\"Large Language Models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m showcase impressive capabilities but encounter challenges like \u001b[0m\n",
              "\u001b[32mhallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented \u001b[0m\n",
              "\u001b[32mGeneration \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m has emerged as a promising solution by incorporating knowledge from external databases. This \u001b[0m\n",
              "\u001b[32menhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for\u001b[0m\n",
              "\u001b[32mcontinuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' \u001b[0m\n",
              "\u001b[32mintrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper \u001b[0m\n",
              "\u001b[32moffers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, \u001b[0m\n",
              "\u001b[32mand the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the \u001b[0m\n",
              "\u001b[32mretrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies \u001b[0m\n",
              "\u001b[32membedded in each of these critical components, providing a profound understanding of the advancements in RAG \u001b[0m\n",
              "\u001b[32msystems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article\u001b[0m\n",
              "\u001b[32mdelineates the challenges currently faced and points out prospective avenues for research and development.\"\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2024-03-27'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation for Large Language Models: A Survey'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Wang, Haofen Wang'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">\"Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">embedded in each of these critical components, providing a profound understanding of the advancements in RAG </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">delineates the challenges currently faced and points out prospective avenues for research and development.\"</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "from langchain_community.document_loaders import ArxivLoader\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
        ")\n",
        "\n",
        "## pick some papers and add them to the list\n",
        "\n",
        "print(\"Loading Documents\")\n",
        "docs = [\n",
        "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
        "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
        "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
        "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
        "    ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
        "    ArxivLoader(query=\"2306.05685\").load(), ## LLM-as-a-Judge\n",
        "    ArxivLoader(query=\"2312.10997\").load(), ## new paper->Retrieval-Augmented Generation for LLMs: A Survey)\n",
        "    #ArxivLoader(query=\"2509.14277\").load(),\n",
        "\n",
        "    ## Some longer papers\n",
        "    # ArxivLoader(query=\"2210.03629\").load(),  ## ReAct Paper\n",
        "    # ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
        "    # ArxivLoader(query=\"2103.00020\").load(),  ## CLIP Paper\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "## This is a standard string in papers.\n",
        "for doc in docs:\n",
        "    content = json.dumps(doc[0].page_content)\n",
        "    if \"References\" in content:\n",
        "        doc[0].page_content = content[:content.index(\"References\")]\n",
        "\n",
        "## Splitting  the documents and also filter out stubs (overly short chunks)\n",
        "print(\"Chunking Documents\")\n",
        "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
        "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
        "\n",
        "## Making some custom Chunks\n",
        "doc_string = \"Available Documents:\"\n",
        "doc_metadata = []\n",
        "for chunks in docs_chunks:\n",
        "    metadata = getattr(chunks[0], 'metadata', {})\n",
        "    doc_string += \"\\n - \" + metadata.get('Title')\n",
        "    doc_metadata += [str(metadata)]\n",
        "\n",
        "extra_chunks = [doc_string] + doc_metadata\n",
        "\n",
        "## Printing out some summary information for reference\n",
        "pprint(doc_string, '\\n')\n",
        "for i, chunks in enumerate(docs_chunks):\n",
        "    print(f\"Document {i}\")\n",
        "    print(f\" - # Chunks: {len(chunks)}\")\n",
        "    print(f\" - Metadata: \")\n",
        "    pprint(chunks[0].metadata)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbWDEHxrg_oD"
      },
      "source": [
        "Task 2: Construct Your Document Vector Stores\n",
        "Now that we have all of the components, we can go ahead and create indices surrounding them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s02di20RhIUV",
        "outputId": "244b6e59-c6db-4ad0-ce9a-c64e1aa09605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructing Vector Stores\n",
            "CPU times: user 1.38 s, sys: 71.2 ms, total: 1.45 s\n",
            "Wall time: 29.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "print(\"Constructing Vector Stores\")\n",
        "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
        "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6sBpStJhLhO"
      },
      "source": [
        "From there, we can combine our indices into a single one using the following utility:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkxhxoUthO_9",
        "outputId": "c76c5b7f-ee54-445f-f6a3-ab5dc93463b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Constructed aggregate docstore with 361 chunks\n"
          ]
        }
      ],
      "source": [
        "from faiss import IndexFlatL2\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "embed_dims = len(embedder.embed_query(\"test\"))\n",
        "def default_FAISS():\n",
        "    '''Useful utility for making an empty FAISS vectorstore'''\n",
        "    return FAISS(\n",
        "        embedding_function=embedder,\n",
        "        index=IndexFlatL2(embed_dims),\n",
        "        docstore=InMemoryDocstore(),\n",
        "        index_to_docstore_id={},\n",
        "        normalize_L2=False\n",
        "    )\n",
        "\n",
        "def aggregate_vstores(vectorstores):\n",
        "    ## Initialize an empty FAISS Index and merge others into it\n",
        "    ## using default_faiss for simplicity,\n",
        "    agg_vstore = default_FAISS()\n",
        "    for vstore in vectorstores:\n",
        "        agg_vstore.merge_from(vstore)\n",
        "    return agg_vstore\n",
        "\n",
        "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
        "docstore = aggregate_vstores(vecstores)\n",
        "\n",
        "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is where we finally train our PCA on our real data."
      ],
      "metadata": {
        "id": "Hf6YMudEhG6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we've merged everything into our 'docstore', we need to\n",
        "# pull out all those 4096-dim NVIDIA vectors. This lets us train\n",
        "# our bridge to compress our specific research papers into 16 qubits.\n",
        "\n",
        "print(\"---Priming Our Quantum Bridge ---\")\n",
        "\n",
        "# grabbing every single vector from our unified FAISS index\n",
        "all_vectors = docstore.index.reconstruct_n(0, docstore.index.ntotal)\n",
        "\n",
        "# fit our bridge. This is the moment our classical\n",
        "# embeddings are mapped to our quantum circuit's rotation angles.\n",
        "bridge.fit(all_vectors)\n",
        "\n",
        "print(f\"✅ Our bridge is now tuned to all {len(all_vectors)} document chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76j-gNuDhH86",
        "outputId": "4392f381-50c2-4271-ac9c-85412e67f3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Priming Our Quantum Bridge ---\n",
            "Fitting Quantum Bridge: 4096 dims -> 16 qubits\n",
            "✅ Quantum Bridge: Successfully fitted to data.\n",
            "✅ Our bridge is now tuned to all 361 document chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we move to Step 5.2: The Quantum Re-ranker Function. This is where we combine everything: we'll take the \"candidates\" that FAISS finds and use our 16-qubit circuit to determine which ones actually have the best quantum overlap with our query."
      ],
      "metadata": {
        "id": "Qt9mlgCCheQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5.2: The Quantum Re-ranker Function"
      ],
      "metadata": {
        "id": "UvzoEW99hqU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this code is also fuctional but, it takes too long to respond to queries\n",
        "'''def quantum_rerank(query, documents, top_k=5):\n",
        "    \"\"\"\n",
        "    Our hybrid re-ranking logic. We take the 'rough' results from FAISS,\n",
        "    run them through our 16-qubit kernel, and pick the true winners.\n",
        "    \"\"\"\n",
        "    if not documents:\n",
        "        return []\n",
        "\n",
        "    # 1. First, we get our query's 4096-dim embedding\n",
        "    query_vec = embedder.embed_query(query)\n",
        "\n",
        "    # 2. We grab the embeddings for our candidate chunks\n",
        "    doc_texts = [d.page_content for d in documents]\n",
        "    doc_vecs = embedder.embed_documents(doc_texts)\n",
        "\n",
        "    # 3. We use our primed bridge to compress them into 16-qubit rotation angles\n",
        "    q_angles = bridge.transform([query_vec])[0]\n",
        "    d_angles_list = bridge.transform(doc_vecs)\n",
        "\n",
        "    # 4. Now we loop through and calculate the quantum similarity for each\n",
        "    scored_docs = []\n",
        "    for i, doc in enumerate(documents):\n",
        "        # We're calling our circuit here to measure the state overlap\n",
        "        score = get_quantum_similarity(q_angles, d_angles_list[i])\n",
        "        scored_docs.append((score, doc))\n",
        "\n",
        "    # 5. We sort them so the highest quantum overlap comes first\n",
        "    scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # We return only the top_k chunks for our LLM to read\n",
        "    return [doc for score, doc in scored_docs[:top_k]]\n",
        "\n",
        "print(\"✅ Our Quantum Re-ranker function is live.\")'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "fkaYbCrvhrAI",
        "outputId": "51d91a32-3ffc-490f-9b59-468252a4c9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'def quantum_rerank(query, documents, top_k=5):\\n    \"\"\"\\n    Our hybrid re-ranking logic. We take the \\'rough\\' results from FAISS,\\n    run them through our 16-qubit kernel, and pick the true winners.\\n    \"\"\"\\n    if not documents:\\n        return []\\n\\n    # 1. First, we get our query\\'s 4096-dim embedding\\n    query_vec = embedder.embed_query(query)\\n\\n    # 2. We grab the embeddings for our candidate chunks\\n    doc_texts = [d.page_content for d in documents]\\n    doc_vecs = embedder.embed_documents(doc_texts)\\n\\n    # 3. We use our primed bridge to compress them into 16-qubit rotation angles\\n    q_angles = bridge.transform([query_vec])[0]\\n    d_angles_list = bridge.transform(doc_vecs)\\n\\n    # 4. Now we loop through and calculate the quantum similarity for each\\n    scored_docs = []\\n    for i, doc in enumerate(documents):\\n        # We\\'re calling our circuit here to measure the state overlap\\n        score = get_quantum_similarity(q_angles, d_angles_list[i])\\n        scored_docs.append((score, doc))\\n\\n    # 5. We sort them so the highest quantum overlap comes first\\n    scored_docs.sort(key=lambda x: x[0], reverse=True)\\n\\n    # We return only the top_k chunks for our LLM to read\\n    return [doc for score, doc in scored_docs[:top_k]]\\n\\nprint(\"✅ Our Quantum Re-ranker function is live.\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Our Optimized Quantum Re-ranker code ---\n",
        "\n",
        "@qml.qnode(dev)\n",
        "def batched_quantum_kernel(query_angle, doc_angles):\n",
        "    \"\"\"\n",
        "    This is our speed-boosted circuit. By passing a list of doc_angles,\n",
        "    PennyLane and IBM Cloud process the entire batch in one go.\n",
        "    \"\"\"\n",
        "    # 1. Encode our query (this is broadcasted automatically)\n",
        "    qml.AngleEmbedding(query_angle, wires=range(16), rotation='X')\n",
        "\n",
        "    # 2. Encode our batch of documents using the adjoint (inverse)\n",
        "    # The 'doc_angles' here is a matrix of (20, 16)\n",
        "    qml.adjoint(qml.AngleEmbedding)(doc_angles, wires=range(16), rotation='X')\n",
        "\n",
        "    # 3. We get a probability vector for every document in the batch\n",
        "    return qml.probs(wires=range(16))\n",
        "\n",
        "def quantum_rerank(query, documents, top_k=5):\n",
        "\n",
        "    if not documents:\n",
        "        return []\n",
        "\n",
        "    # Prepare our vectors as we did before\n",
        "    query_vec = embedder.embed_query(query)\n",
        "    doc_texts = [d.page_content for d in documents]\n",
        "    doc_vecs = embedder.embed_documents(doc_texts)\n",
        "\n",
        "    # Compress them using our 16-qubit bridge\n",
        "    q_angles = bridge.transform([query_vec])[0]\n",
        "    d_angles_batch = bridge.transform(doc_vecs)\n",
        "\n",
        "    # --- SPEED FIX: One single call to the QPU instead of 20 ---\n",
        "    print(f\"---Broadcasting Batch Job to ibm_qpu (Batch Size: {len(documents)}) ---\")\n",
        "    all_probs = batched_quantum_kernel(q_angles, d_angles_batch)\n",
        "\n",
        "    # We grab the first column (the probability of the |0...0> state) for all docs\n",
        "    scores = all_probs[:, 0]\n",
        "\n",
        "    # Map the scores back to our documents and sort\n",
        "    scored_docs = list(zip(scores, documents))\n",
        "    scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    return [doc for score, doc in scored_docs[:top_k]]\n",
        "\n",
        "print(\"Speed Patch Applied: Sequential loops replaced with Batch Processing.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A5a-Mph7RX8",
        "outputId": "2aca689d-74d4-43e2-e697-74f27a99d0b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speed Patch Applied: Sequential loops replaced with Batch Processing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdNVc16rhS-W"
      },
      "source": [
        " Task 3:Implement Your RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytd7kMd9hYJY",
        "outputId": "cd6f7fc2-2345-4e8a-f209-d59286859f92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Our full Hybrid Chain is now active and defined.\n",
            "Testing our brain with: Explain the two main components of the RAG framework and how they interact according to the papers.\n",
            "\n",
            "---Broadcasting Batch Job to ibm_qpu (Batch Size: 20) ---\n",
            " The two main components of the RAG (Retrieval-Augmented Generation) framework are \"Retrieval\" and \"Generation.\"\n",
            "\n",
            "- Retrieval: This component is responsible for searching relevant documents in external databases based on the input. It uses various optimization methods like indexing, query, and embedding optimization to enhance the retrieval process.\n",
            "\n",
            "- Generation: After the retrieval phase, the model uses the fetched documents to generate appropriate responses. This process involves post-retrieval processing and fine-tuning of the large language model (LLM).\n",
            "\n",
            "The \"Retrieval\" and \"Generation\" components collaborate in a way that the Retrieval component provides context-specific documents, and the Generation component utilizes this information to generate precise and accurate responses.\n",
            "\n",
            "Sources:\n",
            "- Retrieval-Augmented Generation for Large Language Models: A Survey\n",
            "- Paper title abbreviated to follow guidelines\n",
            "\n",
            "Sources:\n",
            "- Retrieval-Augmented Generation for Large Language Models: A Survey (2021)\n",
            "- Haofen Wang, Zhengyan Zhou, Xipeng Qiu, Tianyu Zhang, “RAG-Survey”; available at https://github.com/Tongji-KGLLM/RAG-Survey"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_transformers import LongContextReorder\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "import gradio as gr\n",
        "from functools import partial\n",
        "from operator import itemgetter\n",
        "\n",
        "#classical reranking Rag chain code\n",
        "'''# NVIDIAEmbeddings.get_available_models()\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\",api_key=userdata.get(\"NVIDIA_API_KEY\"))\n",
        "# ChatNVIDIA.get_available_models()\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\",api_key=userdata.get(\"NVIDIA_API_KEY\"))\n",
        "# instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
        "\n",
        "convstore = default_FAISS()\n",
        "\n",
        "def save_memory_and_get_output(d, vstore):\n",
        "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
        "    vstore.add_texts([\n",
        "        f\"User previously responded with {d.get('input')}\",\n",
        "        f\"Agent previously responded with {d.get('output')}\"\n",
        "    ])\n",
        "    return d.get('output')\n",
        "\n",
        "initial_msg = (\n",
        "    \"Hello! I am a document chat agent here to help the user!\"\n",
        "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
        ")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"You are a document-based question answering assistant.\\n\\n\"\n",
        "\n",
        "     \"You have access to retrieved content from user-uploaded documents.\\n\"\n",
        "     \"You must follow ALL rules below strictly:\\n\\n\"\n",
        "\n",
        "     \"RULES FOR ANSWERING:\\n\"\n",
        "     \"1. Use the retrieved document context as the primary source of truth.\\n\"\n",
        "     \"2. If a statement is supported by the retrieved documents, include it in the answer.\\n\"\n",
        "     \"3. If a statement is NOT supported by the retrieved documents but is common knowledge, \"\n",
        "     \"you MAY include it, but it MUST be explicitly labeled as general knowledge.\\n\"\n",
        "     \"4. NEVER attribute general knowledge to a document.\\n\"\n",
        "     \"5. NEVER fabricate citations.\\n\\n\"\n",
        "\n",
        "     \"SOURCE ATTRIBUTION RULES:\\n\"\n",
        "     \"- At the end of EVERY response, include a section titled exactly:\\n\"\n",
        "     \"  \\\"Sources\\\"\\n\"\n",
        "     \"- Under \\\"Sources\\\", list ONLY the exact titles of documents that directly support the answer.\\n\"\n",
        "     \"- If part of the answer comes from general knowledge, include a bullet:\\n\"\n",
        "     \"  \\\"General knowledge of the language model (not found in uploaded documents)\\\"\\n\"\n",
        "     \"- If NO documents support the answer, list ONLY the general knowledge bullet.\\n\\n\"\n",
        "\n",
        "     \"FORMAT REQUIREMENTS:\\n\"\n",
        "     \"- Write the main answer first.\\n\"\n",
        "     \"- Then write a blank line.\\n\"\n",
        "     \"- Then write \\\"Sources:\\\" on its own line.\\n\"\n",
        "     \"- Then list sources as bullet points.\\n\\n\"\n",
        "\n",
        "     \"User question:\\n{input}\\n\\n\"\n",
        "\n",
        "     \"Conversation history (may be empty):\\n{history}\\n\\n\"\n",
        "\n",
        "     \"Retrieved document context:\\n{context}\\n\\n\"\n",
        "\n",
        "     \"Answer in a clear, precise, and honest tone.\"\n",
        "    ),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "stream_chain = chat_prompt| instruct_llm | StrOutputParser()\n",
        "\n",
        "################################################################################################\n",
        "##Implement the retrieval chain\n",
        "\n",
        "long_reorder = LongContextReorder()\n",
        "\n",
        "def docs2str(docs, max_chars=4000):\n",
        "    \"\"\"Join retrieved docs into a single text blob including simple source titles.\"\"\"\n",
        "    parts = []\n",
        "    for d in docs:\n",
        "        md = getattr(d, \"metadata\", {}) or {}\n",
        "        title = md.get(\"Title\") or md.get(\"title\") or md.get(\"source\") or \"unknown\"\n",
        "        parts.append(f\"[{title}]\\n{d.page_content}\")\n",
        "    text = \"\\n\\n---\\n\\n\".join(parts)\n",
        "    return text[:max_chars]\n",
        "\n",
        "retrieval_chain = (\n",
        "    {'input' : (lambda x: x)}  # input is a raw string\n",
        "    | RunnableAssign({\n",
        "        # Retrieve recent conversational memory from convstore\n",
        "        \"history\": lambda d: docs2str(convstore.similarity_search(d[\"input\"], k=4), max_chars=2000),\n",
        "        # Retrieve relevant document chunks from docstore, reorder long context, and stringify\n",
        "        \"context\": lambda d: docs2str(\n",
        "            long_reorder.transform_documents(docstore.similarity_search(d[\"input\"], k=8)),\n",
        "            max_chars=4000\n",
        "        ),\n",
        "    })\n",
        ")\n",
        "#################################################################################\n",
        "\n",
        "def chat_gen(message, history=[], return_buffer=True):\n",
        "    buffer = \"\"\n",
        "    ## First performing retrieval based on the input message\n",
        "    retrieval = retrieval_chain.invoke(message)\n",
        "    line_buffer = \"\"\n",
        "\n",
        "    ## Then, stream the results of the stream_chain\n",
        "    for token in stream_chain.stream(retrieval):\n",
        "        buffer += token\n",
        "\n",
        "        yield buffer if return_buffer else token\n",
        "\n",
        "    ## Lastly, saving the chat exchange to the conversation memory buffer\n",
        "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
        "\n",
        "\n",
        "## Start of Agent Event Loop\n",
        "test_question = \"Tell me about RAG!\"\n",
        "\n",
        "## Before launching gradio interface,test the working\n",
        "for response in chat_gen(test_question, return_buffer=False):\n",
        "    print(response, end='') '''\n",
        "\n",
        "# ---Our Quantum-Enhanced RAG Chain ---\n",
        "\n",
        "\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\", api_key=userdata.get(\"NVIDIA_API_KEY\"))\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\", api_key=userdata.get(\"NVIDIA_API_KEY\"))\n",
        "convstore = default_FAISS()\n",
        "\n",
        "# save our conversation to memory\n",
        "def save_memory_and_get_output(d, vstore):\n",
        "    vstore.add_texts([\n",
        "        f\"User previously responded with {d.get('input')}\",\n",
        "        f\"Agent previously responded with {d.get('output')}\"\n",
        "    ])\n",
        "    return d.get('output')\n",
        "\n",
        "# Helper to turn our retrieved chunks into a single string for the LLM\n",
        "def docs2str(docs, max_chars=4000):\n",
        "    parts = []\n",
        "    for d in docs:\n",
        "        md = getattr(d, \"metadata\", {}) or {}\n",
        "        title = md.get(\"Title\") or md.get(\"title\") or md.get(\"source\") or \"unknown\"\n",
        "        parts.append(f\"[{title}]\\n{d.page_content}\")\n",
        "    text = \"\\n\\n---\\n\\n\".join(parts)\n",
        "    return text[:max_chars]\n",
        "\n",
        "# This creates the greeting message for our chatbot\n",
        "initial_msg = (\n",
        "    \"Hello! I am a document chat agent here to help our user!\"\n",
        "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help us today?\"\n",
        ")\n",
        "\n",
        "# Our instruction set for the assistant\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"You are a document-based question answering assistant.\\n\\n\"\n",
        "     \"You have access to retrieved content from user-uploaded documents.\\n\"\n",
        "     \"You must follow ALL rules below strictly:\\n\\n\"\n",
        "     \"RULES FOR ANSWERING:\\n\"\n",
        "     \"1. Use the retrieved document context as the primary source of truth.\\n\"\n",
        "     \"2. If a statement is supported by the retrieved documents, include it in the answer.\\n\"\n",
        "     \"3. If a statement is NOT supported by the retrieved documents but is common knowledge, \"\n",
        "     \"you MAY include it, but it MUST be explicitly labeled as general knowledge.\\n\"\n",
        "     \"4. NEVER attribute general knowledge to a document.\\n\"\n",
        "     \"5. NEVER fabricate citations.\\n\\n\"\n",
        "     \"SOURCE ATTRIBUTION RULES:\\n\"\n",
        "     \"- At the end of EVERY response, include a section titled exactly:\\n\"\n",
        "     \"  \\\"Sources\\\"\\n\"\n",
        "     \"- Under \\\"Sources\\\", list ONLY the exact titles of documents that directly support the answer.\\n\"\n",
        "     \"- If part of the answer comes from general knowledge, include a bullet:\\n\"\n",
        "     \"  \\\"General knowledge of the language model (not found in uploaded documents)\\\"\\n\"\n",
        "     \"- If NO documents support the answer, list ONLY the general knowledge bullet.\\n\\n\"\n",
        "     \"FORMAT REQUIREMENTS:\\n\"\n",
        "     \"- Write the main answer first.\\n\"\n",
        "     \"- Then write a blank line.\\n\"\n",
        "     \"- Then write \\\"Sources:\\\" on its own line.\\n\"\n",
        "     \"- Then list sources as bullet points.\\n\\n\"\n",
        "     \"User question:\\n{input}\\n\\n\"\n",
        "     \"Conversation history (may be empty):\\n{history}\\n\\n\"\n",
        "     \"Retrieved document context:\\n{context}\\n\\n\"\n",
        "     \"Answer in a clear, precise, and honest tone.\"\n",
        "    ),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "# final response\n",
        "\n",
        "stream_chain = chat_prompt | instruct_llm | StrOutputParser()\n",
        "\n",
        "# --- Our Hybrid Retrieval Logic ---\n",
        "# We grab 20 candidates and let our 16-qubit re-ranker find the best 5\n",
        "retrieval_chain = (\n",
        "    {'input' : (lambda x: x)}\n",
        "    | RunnableAssign({\n",
        "        \"history\": lambda d: docs2str(convstore.similarity_search(d[\"input\"], k=4), max_chars=2000),\n",
        "        \"context\": lambda d: docs2str(\n",
        "            quantum_rerank(\n",
        "                d[\"input\"],\n",
        "                docstore.similarity_search(d[\"input\"], k=20)\n",
        "            ),\n",
        "            max_chars=4000\n",
        "        ),\n",
        "    })\n",
        ")\n",
        "\n",
        "# Our final function that Gradio will call\n",
        "def chat_gen(message, history=[], return_buffer=True):\n",
        "    buffer = \"\"\n",
        "    # We run our quantum-enhanced retrieval first\n",
        "    retrieval = retrieval_chain.invoke(message)\n",
        "\n",
        "    # Then we stream the LLM's response based on those results\n",
        "    for token in stream_chain.stream(retrieval):\n",
        "        buffer += token\n",
        "        yield buffer if return_buffer else token\n",
        "\n",
        "    # Save the exchange to our memory\n",
        "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
        "\n",
        "print(\"✅ Our full Hybrid Chain is now active and defined.\")\n",
        "\n",
        "# --- Quick Test Run ---\n",
        "test_question = \"Explain the two main components of the RAG framework and how they interact according to the papers.\"\n",
        "print(f\"Testing our brain with: {test_question}\\n\")\n",
        "\n",
        "for response in chat_gen(test_question, return_buffer=False):\n",
        "    print(response, end='')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Force-installing setuptools (needed for Python 3.12) and the IBM runtime\n",
        "%pip install -qU setuptools qiskit-ibm-runtime pennylane-qiskit"
      ],
      "metadata": {
        "id": "eOCE3Jz-pP5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba6da3b-1dd0-404c-9c7c-428d0ef15f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m378.6/378.6 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.8/75.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "from qiskit_ibm_runtime import QiskitRuntimeService\n",
        "\n",
        "# --- IBM CLOUD PRODUCTION CONFIGURATION ---\n",
        "\n",
        "# This is our unique Cloud Resource Name for our IBM Cloud instance\n",
        "MY_CRN = userdata.get('IBM_CRN')\n",
        "\n",
        "try:\n",
        "\n",
        "    # We pass our CRN directly into the 'instance' parameter.\n",
        "    service = QiskitRuntimeService(\n",
        "        channel=\"ibm_cloud\",\n",
        "        token=userdata.get('IBM_TOKEN'),\n",
        "        instance=MY_CRN\n",
        "    )\n",
        "\n",
        "    # 2. Locking onto the 133-qubit(in this case) hardware\n",
        "    qpu_backend = service.backend(\"ibm_torino\")\n",
        "\n",
        "    # 3. THE HARDWARE SWITCH:\n",
        "\n",
        "    # Swapping our local 'default.qubit' for the remote 16-qubit.\n",
        "\n",
        "    dev = qml.device('qiskit.remote', wires=16, backend=qpu_backend)\n",
        "    ##for using simulator make above line comment and uncomment below line.\n",
        "\n",
        "    #dev = qml.device(\"lightning.qubit\", wires=16)\n",
        "\n",
        "    # 4. Redefining our circuit to use the real hardware device\n",
        "    @qml.qnode(dev)\n",
        "    def quantum_kernel_circuit(x1, x2):\n",
        "        # Angle Embedding for our 16 compressed dimensions\n",
        "        qml.AngleEmbedding(x1, wires=range(16), rotation='X')\n",
        "        # Adjoint (inverse) for the document chunk to measure overlap\n",
        "        qml.adjoint(qml.AngleEmbedding)(x2, wires=range(16), rotation='X')\n",
        "        return qml.probs(wires=range(16))\n",
        "\n",
        "    print(f\"✅ SYSTEM LIVE ON IBM CLOUD: {qpu_backend.name}\")\n",
        "    print(f\"Qubits: {qpu_backend.num_qubits}\")\n",
        "    print(f\"Status: {qpu_backend.status().status_msg}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Connection Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xqfzxFHonqc",
        "outputId": "9cdc9f99-3e07-4f0c-b2ce-2f349366899d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "qiskit_runtime_service._discover_account:WARNING:2026-01-17 12:53:41,386: Loading account with the given token. A saved account will not be used.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ SYSTEM LIVE ON IBM CLOUD: ibm_torino\n",
            "Qubits: 133\n",
            "Status: active\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6ZIoIWnhcvV"
      },
      "source": [
        "Task 4: Interact With Your Gradio Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "nMbLlBedhfDF",
        "outputId": "4294689f-f112-4821-8447-44660fc1bdcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2790734879.py:5: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching our Quantum-Enhanced Interface...\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://12242341e10c4c913e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://12242341e10c4c913e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://12242341e10c4c913e.gradio.live\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# 1. We update our chatbot component to explicitly use 'messages' type.\n",
        "\n",
        "chatbot = gr.Chatbot(\n",
        "    value=[{\"role\": \"assistant\", \"content\": initial_msg}],\n",
        "    height=600,\n",
        "    type=\"messages\"\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "demo = gr.ChatInterface(\n",
        "    chat_gen,\n",
        "    chatbot=chatbot,\n",
        "    type=\"messages\",\n",
        "    title=\"Hybrid Quantum-Classical RAG\",\n",
        "    description=f\"Active Backend: **ibm_torino** | NVIDIA Embeddings: **nv-embed-v1**\",\n",
        "    theme=\"soft\"\n",
        ").queue()\n",
        "\n",
        "print(\"Launching our Quantum-Enhanced Interface...\")\n",
        "try:\n",
        "    demo.launch(debug=True, share=True)\n",
        "except Exception as e:\n",
        "    demo.close()\n",
        "    print(f\"Interface Error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}